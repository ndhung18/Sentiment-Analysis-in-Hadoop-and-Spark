{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1Puw6GDFrpp",
        "outputId": "7e028678-dc26-463b-b0f1-8b1f3db78626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spark-nlp in /usr/local/lib/python3.10/dist-packages (5.5.1)\n"
          ]
        }
      ],
      "source": [
        "pip install spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x6Na4zPlJWfq"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col, concat_ws, rand\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0LQKGMvDJYfv"
      },
      "outputs": [],
      "source": [
        "spark = sparknlp.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0VsxAw7nwICr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e7ade5-8234-469c-f98c-3ee83dae577a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---------------+--------------------+-----------+--------------------+--------------------+--------------------+\n",
            "|ItemID|Sentiment|SentimentSource|       SentimentText|text_length|               words|      filtered_words|    lemmatized_words|\n",
            "+------+---------+---------------+--------------------+-----------+--------------------+--------------------+--------------------+\n",
            "|    62|        1|   Sentiment140|i always get what...|         27|[i, always, get, ...| [always, get, want]| [always, get, want]|\n",
            "|   194|        1|   Sentiment140|tell  i said happ...|         44|[tell, , i, said,...|[tell, , said, ha...|[tell, , said, ha...|\n",
            "|   436|        0|   Sentiment140|i hope everyone i...|        136|[i, hope, everyon...|[hope, everyone, ...|[hope, everyone, ...|\n",
            "|   474|        0|   Sentiment140|all my friends ar...|        101|[all, my, friends...|[friends, gone, h...|[friend, gone, ha...|\n",
            "|   619|        0|   Sentiment140|bcds closed i gue...|         36|[bcds, closed, i,...|[bcds, closed, gu...|[bcds, closed, gu...|\n",
            "|   712|        0|   Sentiment140|i was want to buy...|         64|[i, was, want, to...|[want, buy, sims,...|[want, buy, sims,...|\n",
            "|   846|        0|   Sentiment140|damn there has to...|         56|[damn, there, has...|[damn, something,...|[damn, something,...|\n",
            "|   907|        1|   Sentiment140|posterous looks l...|        126|[posterous, looks...|[posterous, looks...|[posterous, look,...|\n",
            "|  1042|        1|   Sentiment140|                    |         10|                  []|                  []|                  []|\n",
            "|  1231|        0|   Sentiment140|         hiccuuppsss|         13|       [hiccuuppsss]|       [hiccuuppsss]|       [hiccuuppsss]|\n",
            "|  1455|        0|   Sentiment140|i guess i wouldnt...|         95|[i, guess, i, wou...|[guess, wouldnt, ...|[guess, wouldnt, ...|\n",
            "|  1583|        0|   Sentiment140|i hope justice is...|         30|[i, hope, justice...|[hope, justice, i...|[hope, justice, i...|\n",
            "|  1709|        0|   Sentiment140|i miss guild wars...|        137|[i, miss, guild, ...|[miss, guild, war...|[miss, guild, war...|\n",
            "|  1915|        1|   Sentiment140|finishing up my m...|         97|[finishing, up, m...|[finishing, midte...|[finishing, midte...|\n",
            "|  2026|        1|   Sentiment140|fresh out the sho...|         57|[fresh, out, the,...|[fresh, shower, w...|[fresh, shower, w...|\n",
            "|  2253|        0|   Sentiment140|  have to leave soon|         19|[have, to, leave,...|       [leave, soon]|       [leave, soon]|\n",
            "|  2439|        0|   Sentiment140|i beeeeen sayin w...|        116|[i, beeeeen, sayi...|[beeeeen, sayin, ...|[beeeeen, sayin, ...|\n",
            "|  2539|        0|   Sentiment140|i cant get ma rar...|         58|[i, cant, get, ma...|[cant, get, ma, r...|[cant, get, ma, r...|\n",
            "|  2577|        0|   Sentiment140|i feel like freak...|         28|[i, feel, like, f...|[feel, like, frea...|[feel, like, frea...|\n",
            "|  3566|        0|   Sentiment140|still waiting for...|         40|[still, waiting, ...|[still, waiting, ...|[still, waiting, ...|\n",
            "+------+---------+---------------+--------------------+-----------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- ItemID: integer (nullable = true)\n",
            " |-- Sentiment: integer (nullable = true)\n",
            " |-- SentimentSource: string (nullable = true)\n",
            " |-- SentimentText: string (nullable = true)\n",
            " |-- text_length: integer (nullable = true)\n",
            " |-- words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- filtered_words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- lemmatized_words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read data\n",
        "data = spark.read.parquet(\"/content/drive/MyDrive/cleaned_data/\")\n",
        "data.show()\n",
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupBy(\"Sentiment\").count().show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPBMJsYXlvSM",
        "outputId": "a4312b01-a4af-4833-c620-eb403194b498"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|Sentiment|count |\n",
            "+---------+------+\n",
            "|1        |790018|\n",
            "|0        |788387|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AJtw8KJOwSIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb3ff8c-1fe3-489d-a07a-e674a3c52635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|Sentiment|count |\n",
            "+---------+------+\n",
            "|1        |100000|\n",
            "|0        |100000|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take sample size\n",
        "sample_size = 100000\n",
        "\n",
        "positive_df = data.filter(data[\"Sentiment\"] == \"1\").limit(sample_size)\n",
        "negative_df = data.filter(data[\"Sentiment\"] == \"0\").limit(sample_size)\n",
        "\n",
        "balanced_dataset = positive_df.union(negative_df)\n",
        "balanced_dataset.groupBy(\"Sentiment\").count().show(truncate=False)\n",
        "\n",
        "# Split data\n",
        "trainingData = balanced_dataset.orderBy(rand())\n",
        "(trainData, testData) = trainingData.randomSplit([0.8, 0.2], seed = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIGy_f8uwps1",
        "outputId": "3e7aa9b8-388f-4290-f93e-872d34923b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small_bert_L4_256 download started this may take some time.\n",
            "Approximate size to download 40.5 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "# ClassifierDLApproach + BertEmbeddings configs\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"SentimentText\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "bert_embeddings = BertEmbeddings().pretrained(name='small_bert_L4_256', lang='en') \\\n",
        "    .setInputCols([\"document\", 'token']) \\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings() \\\n",
        "    .setInputCols([\"document\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\") \\\n",
        "    .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "classsifierdl = ClassifierDLApproach() \\\n",
        "    .setInputCols([\"sentence_embeddings\"]) \\\n",
        "    .setOutputCol(\"class\") \\\n",
        "    .setLabelColumn(\"Sentiment\") \\\n",
        "    .setMaxEpochs(10) \\\n",
        "    .setLr(0.001) \\\n",
        "    .setBatchSize(16)\\\n",
        "    .setEnableOutputLogs(True)\\\n",
        "    .setOutputLogsPath('logs')\\\n",
        "    .setVerbose(1)\\\n",
        "    .setValidationSplit(0.2)\\\n",
        "    .setDropout(0.2)\\\n",
        "\n",
        "classsifierdl_pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    bert_embeddings,\n",
        "    embeddingsSentence,\n",
        "    classsifierdl\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifierdl_model = classsifierdl_pipeline.fit(trainData)\n",
        "\n",
        "preds = classifierdl_model.transform(testData)\n",
        "\n",
        "preds_df = preds.select('Sentiment','SentimentText',\"class.result\").toPandas()\n",
        "\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "\n",
        "preds_df['result'] = preds_df['result'].astype(int)\n",
        "\n",
        "print (classification_report(preds_df['result'], preds_df['Sentiment']))"
      ],
      "metadata": {
        "id": "6V-Egj2df_sg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365565ff-0728-41b1-956f-b1307bc6580c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.80      0.76     18213\n",
            "           1       0.82      0.75      0.78     22004\n",
            "\n",
            "    accuracy                           0.77     40217\n",
            "   macro avg       0.77      0.77      0.77     40217\n",
            "weighted avg       0.78      0.77      0.77     40217\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifierdl_model.save(\"/content/classifierdl_model\")"
      ],
      "metadata": {
        "id": "4GdLCdyAktub"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uypemik707N2",
        "outputId": "f8234463-8d83-4d6c-e3e5-2abbc1c9cb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started - epochs: 10 - learning_rate: 0.001 - batch_size: 16 - training_examples: 127836 - classes: 2\n",
            "Epoch 0/10 - 71.49s - loss: 4619.7197 - acc: 0.71590626 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "Epoch 1/10 - 68.64s - loss: 4488.7236 - acc: 0.73769665 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "Epoch 2/10 - 66.44s - loss: 4418.5386 - acc: 0.7510691 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "Epoch 3/10 - 70.56s - loss: 4347.473 - acc: 0.7628536 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "Epoch 4/10 - 71.33s - loss: 4278.4243 - acc: 0.7747684 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "Epoch 5/10 - 71.13s - loss: 4213.424 - acc: 0.7848135 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "Epoch 6/10 - 68.58s - loss: 4151.4697 - acc: 0.7951167 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "Epoch 7/10 - 69.75s - loss: 4091.971 - acc: 0.803691 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "Epoch 8/10 - 66.74s - loss: 4037.07 - acc: 0.8125548 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "Epoch 9/10 - 71.21s - loss: 3982.6367 - acc: 0.8203467 - batches: 7990\n",
            "Quality on validation dataset (20.0%), validation examples = 31958\n",
            "\n"
          ]
        }
      ],
      "source": [
        "log_file_path = '/content/logs/ClassifierDLApproach_62d083ebf2b6.log'\n",
        "\n",
        "with open(log_file_path, 'r') as file:\n",
        "    log_content = file.read()\n",
        "\n",
        "print(log_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PipelineModel.load(\"/content/classifierdl_model\")\n",
        "\n",
        "preds = model.transform(testData)\n",
        "\n",
        "preds_df = preds.select('Sentiment','SentimentText',\"class.result\").toPandas()\n",
        "\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "\n",
        "preds_df['result'] = preds_df['result'].astype(int)\n",
        "\n",
        "print (classification_report(preds_df['result'], preds_df['Sentiment']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yno5-EvQ6GhB",
        "outputId": "57a9958a-fe90-471e-c406-e54d9eeb4d9d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.80      0.76     18171\n",
            "           1       0.82      0.75      0.79     22046\n",
            "\n",
            "    accuracy                           0.78     40217\n",
            "   macro avg       0.78      0.78      0.78     40217\n",
            "weighted avg       0.78      0.78      0.78     40217\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}